{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install .[autogen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load OpenAI config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import oai\n",
    "from flaml.autogen.agentchat import AssistantAgent, UserProxyAgent\n",
    "from flaml.autogen.agent.roleplay_agent import Message\n",
    "from flaml.autogen.agentchat.chat import Chat\n",
    "\n",
    "OAI_CONFIG_LIST = \"../OAI_CONFIG_LIST.json\"\n",
    "config_list = oai.config_list_from_json(\n",
    "        OAI_CONFIG_LIST,\n",
    "    )\n",
    "config_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create roleplay chat and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_runner = AssistantAgent(\n",
    "        name=\"code_runner_tool\",\n",
    "        system_message=\"python code runner tool. Automatically run python code block and return the result.\",\n",
    "        code_execution_config={\"use_docker\": False},\n",
    "        oai_config=False)\n",
    "    \n",
    "alice = AssistantAgent(\n",
    "        name=\"Alice\",\n",
    "        system_message=\"A helpful AI assistant that resolve every task using python script.\",\n",
    "        oai_config={\n",
    "            \"temperature\": 0.5,\n",
    "            \"config_list\": config_list,\n",
    "            \"model\": 'gpt-4',\n",
    "            })\n",
    "\n",
    "grounding_agent = AssistantAgent(\n",
    "        name=\"grounding_agent\",\n",
    "        system_message=\"A helpful AI assistant that fact check any comments from Alice\",\n",
    "        oai_config={\n",
    "                \"temperature\": 0,\n",
    "                \"config_list\": config_list,\n",
    "                \"model\": 'gpt-4',\n",
    "                })\n",
    "\n",
    "    \n",
    "human = UserProxyAgent(\n",
    "        name=\"Human\",\n",
    "        system_message=\"A human admin.\",\n",
    "        human_input_mode=\"ALWAYS\")\n",
    "chat = Chat(human, [code_runner, alice, grounding_agent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casual chat with the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Human]: Hey @Alice, can you find the most recent flaml PR using python?\n",
      "selected agent: Alice\n",
      "[Alice]: Sure! Let me fetch the most recent flaml PR for you using Python.\n",
      "selected agent: grounding_agent\n",
      "[grounding_agent]: Hold on, Alice. Let me fact check your statement.\n",
      "selected agent: grounding_agent\n",
      "[grounding_agent]: According to my fact-checking, Alice's statement is accurate. She can indeed find the most recent flaml PR using Python.\n",
      "selected agent: Alice\n",
      "[Alice]: Thank you for fact-checking, grounding_agent. Now, let me find the most recent flaml PR using Python for you, Human.\n",
      "\n",
      "To accomplish this task, I will use the GitHub API to search for the most recent PR related to the flaml repository.\n",
      "\n",
      "Here's the Python script to find the most recent flaml PR:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Define the repository name and owner\n",
      "repository = \"flaml\"\n",
      "owner = \"microsoft\"\n",
      "\n",
      "# Make a GET request to the GitHub API to fetch the most recent PR\n",
      "response = requests.get(f\"https://api.github.com/repos/{owner}/{repository}/pulls\")\n",
      "\n",
      "# Convert the response to JSON format\n",
      "pr_data = response.json()\n",
      "\n",
      "# Get the most recent PR\n",
      "most_recent_pr = pr_data[0]\n",
      "\n",
      "# Extract the PR number and title\n",
      "pr_number = most_recent_pr[\"number\"]\n",
      "pr_title = most_recent_pr[\"title\"]\n",
      "\n",
      "# Print the most recent PR information\n",
      "print(f\"The most recent flaml PR is #{pr_number}: {pr_title}\")\n",
      "```\n",
      "\n",
      "You can run this script in your Python environment, and it will fetch and print the most recent flaml PR.\n",
      "\n",
      "Let me know if you need any further assistance!\n",
      "selected agent: Human\n",
      "[Human]: @code_runner, run Alice's code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:SIGALRM is not supported on Windows. No timeout will be enforced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected agent: code_runner_tool\n",
      "found code! executing...\n",
      "[code_runner_tool]: exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "The most recent flaml PR is #1155: Optiguide_agentchat\n",
      "\n",
      "selected agent: Alice\n",
      "[Alice]: The most recent flaml PR is #1155: Optiguide_agentchat.\n",
      "\n",
      "Is there anything else I can assist you with?\n",
      "selected agent: Human\n",
      "[Human]: \n",
      "selected agent: Human\n"
     ]
    }
   ],
   "source": [
    "input = {\n",
    "        \"role\": human.name,\n",
    "        \"content\": \"Hey @Alice, can you find the most recent flaml PR using python?\"\n",
    "}\n",
    "chat.send(input, max_round=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
